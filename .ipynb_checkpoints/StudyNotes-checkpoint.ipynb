{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This script summarize the learning notes of neuron network and deep learning\n",
    "(The main reference is \"Neural Networks and Deep Learning\" written by Michael Nielsen)\n",
    "     \n",
    "To my own understanding, neural network is a method help people constructing the estimation function s.t.  \n",
    "For a given input (e.g. image with hand written digit), the estimation function can provide you the expected output you want      (e.g. digit in that image)     \n",
    "\n",
    "The difficulty is the constructing process, the relation between output and input is not always apparent.    \n",
    "Therefore, we have to use a structure of neuron network and a training process to find the best estimation function.    \n",
    "Here is an example of neuron network:  \n",
    "<img src=\"https://miro.medium.com/max/1400/1*ZB6H4HuF58VcMOWbdpcRxQ.png\" height=\"500\" width=\"600\">\n",
    "(Image source: Jayesh Bapu Ahire https://medium.com/coinmonks/the-artificial-neural-networks-handbook-part-1-f9ceb0e376b4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sigmoid function\n",
    "Sigmoid function is frequently used in neuron network built-up, by reviewing it, we can understand how does the input and output look like for a single neuron\n",
    "\n",
    "e.g. The following figure shows a single neuron in network, x1~x3 is going to be the input, sigmoid function is frequently used to calculate the output value\n",
    "<img src=\".\\Images\\SingleNeuron.PNG\" height=\"200\" width=\"200\">\n",
    "\n",
    "By making use of Sigmoid function, the output of this neuron will be defined as:\n",
    "$$ \\sigma(z)=\\frac {1}{1+e^{-z}}$$\n",
    "in which, $\\sigma(z)$ is the output and z depends on inputs of the neuron\n",
    "\n",
    "Furthermore, z depends on input weights $w$'s and bias $b$ s.t.\n",
    "$$ z = w \\cdot x + b = \\sum_j{w_jx_j} + b$$\n",
    "\n",
    "\n",
    "### 1.1 The shape of Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26213e36860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "left_limit = -8.0\n",
    "right_limit =8.0\n",
    "z= np.arange(left_limit,right_limit+0.1,0.1)\n",
    "sigma=1.0/(1.0+np.exp(-z))\n",
    "\n",
    "plt.plot(z, sigma, 'r-')\n",
    "plt.axis([left_limit,right_limit, -0.1, 1.25])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the plot we could tell there are at least these advantages of using this function:\n",
    "- 1. $ 0<\\sigma(z)<1$ , which ganrantee it's a well-defined probability function\n",
    " 2. $\\frac{\\sigma(z)+\\sigma(-z)}{2}=\\frac{1}{2}$, which tells us this function is symmetric with respect to straight line y=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Derivative of Sigmoid function\n",
    "Next, let's take a look at the shape of its derivative:\n",
    "$$ {\\sigma}^{'}(z)=\\frac {1}{(1+e^{-z})(1+e^{z})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26209ef0fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sigma_p=1.0/((1.0+np.exp(-z))*(1.0+np.exp(z)))\n",
    "plt.plot(z, sigma_p, 'b-')\n",
    "plt.axis([left_limit,right_limit,0.0,0.3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Cost function and training of neuron network\n",
    "Assume we have a cost function for so constructed neuron network, and our training goal is to minimize the output of this cost function. During the training, we tune the weights and biases to minimize the cost function until we get best results for all of the input data. \n",
    "\n",
    "Let's take a look at one example:\n",
    "                    $$ c(w,b) = \\frac{1}{2n} \\sum_x{||y(x)-a||^2} \\tag{Eq-2.1}$$\n",
    "in which x is the input of during training, the defined cost function sums the error between estimation $y(x)$ given by network and the expected value $a$ for that input $x$, the errors are summed over the whole domain of input sets, e.g. all the $x$'s\n",
    "\n",
    "And we should expect to see that such combination of weights and biases will give the best results for any new given input. \n",
    "\n",
    "### 2.2 Gradient descent \n",
    "The training process of neuron network relies a lot on gradient descent. To my understanding, gradient descent is taking a strategy s.t. cost function can always decrease the most\n",
    "\n",
    "For example, apparently we have:\n",
    "$$\\Delta c = \\frac{\\partial c}{\\partial w} \\Delta w + \\frac{\\partial c}{\\partial b} \\Delta b$$\n",
    "The training process consists of many iteration substeps, in each substep we are going to make a small change to our neuron network by $(\\Delta w ,\\Delta b)$. During one iteration, if the step size (e.g. $\\sqrt {||\\Delta w||^2 + ||\\Delta b||^2 }$) has a fixed value, mathmetically we can prove that if $(\\Delta w, \\Delta b)$ follows the direction of gradient of cost function at point $(w,b)$ we will decrease the most, e.g.\n",
    "$$ (\\Delta w, \\Delta b) = -\\eta (\\frac{\\partial c}{\\partial w}, \\frac{\\partial c}{\\partial b}) $$\n",
    "in which we use $\\eta$ to control the step size and a negative symbol to guarantee the decrease of cost function.\n",
    "\n",
    "To summarize, let \n",
    "$$v = (w,b), \\nabla c = (\\frac{\\partial c}{\\partial w}, \\frac{\\partial c}{\\partial b})$$\n",
    "then:\n",
    "$$v^{'} = v - \\eta \\nabla c$$\n",
    "$$c^{'} = c - \\eta || \\nabla c ||^2 $$\n",
    "in each iteration, we update $v$ by $- \\eta \\nabla c$, and cost function decrease by $- \\eta || \\nabla c ||^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stochastic gradient descent\n",
    "In general, the training set can be very big (e.g. there are too many $x$'s). Recall our cost function as defined in (Eq-2.1), it relies on all the inputs. Thus it will cost too much to calculate gradient using all the $x$'s in every iterations. \n",
    "Therefore, a strategy of stochastic gradient descent is thus introduced. The key idea is similar to random sampling in statistics. A randomly chosen small sample can be used to estimate the behavior of the whole sample space, s.t.\n",
    "\n",
    "$$c^{'} = c - \\eta || \\frac{{\\nabla} c_m}{m} ||^2 $$\n",
    "in which ${\\nabla} c_m = \\sum\\limits_{x \\, in \\, m}{\\frac{\\partial c}{\\partial v}} $\n",
    "\n",
    "$$v^{'} = v - \\frac{\\eta}{m} {\\nabla} c_m $$\n",
    "\n",
    "Sometimes, this $\\frac{1}{m}$ term can be omitted, because it does not make any difference to the iteration process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  One example: Implementation of previous principles \n",
    "Let's make use of previous principles to construct a simple neuron network and study its performance.   \n",
    "Before goes into details of neural network training, we have to take a look at loading MNIST data as preparation.\n",
    "We will later give the implementation of our network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1  Load MNIST data\n",
    "Use pickle to transform original binary data into numpy array\n",
    "(Copied from Michael's code with no changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    #####################################################################################################\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data, the validation data, and the test \n",
    "    data. The ``training_data`` is returned as a tuple with two entries. The first entry contains the \n",
    "    actual training images. This is a numpy ndarray with 50,000 entries.  Each entry is, in turn, a numpy\n",
    "    ndarray with 784 values, representing the 28 * 28 = 784 pixels in a single MNIST image. The second \n",
    "    entry in the ``training_data`` tuple is a numpy ndarray containing 50,000 entries. Those entries are\n",
    "    just the digit values (0...9) for the corresponding images contained in the first entry of the tuple. \n",
    "    The ``validation_data`` and ``test_data`` are similar, except each contains only 10,000 images. \n",
    "    This is a nice data format, but for use in neural networks it's helpful to modify the format of the \n",
    "    ``training_data`` a little. That's done in the wrapper function ``load_data_wrapper()``, see below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('.\\Data\\mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():    \n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data, test_data)``. Based on ``load_data``,\n",
    "    but the format is more convenient for use in our implementation of neural networks. In particular, \n",
    "    ``training_data`` is a list containing 50,000 2-tuples ``(x, y)``.  ``x`` is a 784-dimensional \n",
    "    numpy.ndarray containing the input image.  ``y`` is a 10-dimensional numpy.ndarray representing the \n",
    "    unit vector corresponding to the correct digit for ``x``. ``validation_data`` and ``test_data`` are \n",
    "    lists containing 10,000 2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional numpy.ndarry \n",
    "    containing the input image, and ``y`` is the corresponding classification, i.e., the digit values \n",
    "    (integers) corresponding to ``x``. Obviously, this means we're using slightly different formats for \n",
    "    the training data and the validation / test data. These formats turn out to be the most convenient \n",
    "    for use in our neural network code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth position and zeroes elsewhere.  \n",
    "    This is used to convert a digit (0...9) into a corresponding desired output from the neural network.\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of sample image from MNIST dataset\n",
    "* Use one image from MNIST dataset to give direct visualization, which can help understanding the structure of the MNIST better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hand-written digit in this image is 5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f4741740f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_d, va_d, te_d = load_data()\n",
    "data_no = 31520  #No. of labeled data, should be 0-49999, a total of 50,000 images in this training set\n",
    "image= np.reshape(tr_d[0][data_no], (28, 28)) \n",
    "digit_in_image = tr_d[1][data_no]\n",
    "print(\"The hand-written digit in this image is\", digit_in_image)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Implementation of neuron network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        '''Initiate the network by number of neurons in each layer, for example sizes = [2,3,1] will create \n",
    "        a network with input layer of 2 neurons, one hidden layer of 3 neurons and output layer of 1 neuron\n",
    "        '''\n",
    "        self.num_layers  = len(sizes)\n",
    "        self.size    = sizes\n",
    "         #Initiate biases array, random number following Gaussian distributions with mean 0 and \n",
    "         # standard deviation 1\n",
    "        self.biases  = [ np.random.randn(y,1) for y in sizes[1:] ]\n",
    "         #Initiate weights matrix, pay attention to the matrix dimensions\n",
    "        self.weights = [ np.random.randn(y,x) for x,y in zip(sizes[:-1],sizes[1:])] \n",
    "\n",
    "    def feedforward(self,a):\n",
    "        '''Return the output of the network with input a'''\n",
    "        for w,b in zip(self.weights, self.biases):\n",
    "            a = self.sigmoid(np.dot(w,a) + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self , training_data , epochs , mini_batch_size , eta, test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient descent. The ``training_data `` is \n",
    "        a list of tuples ``(x, y)`` representing the training inputs and the desired outputs. The other \n",
    "        non-optional parameters are self-explanatory. If ``test_data `` is provided then the network will be \n",
    "        evaluated against the test data after each epoch, and partial progress printed out. This is useful \n",
    "        for tracking progress , but slows things down substantially.\"\"\"\n",
    "        if test_data:\n",
    "            n_test = len(list(test_data))\n",
    "            n = len(list(training_data))\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [ training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size) ]\n",
    "        for mini_batch in mini_batches:\n",
    "            self.update_mini_batch(mini_batch, eta)\n",
    "        if test_data:\n",
    "            print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "        else:\n",
    "            print( \"Epoch {0} complete\".format(j))\n",
    "            \n",
    "    def update_mini_batch(self , mini_batch , eta):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient descent using backpropagation to a \n",
    "        single mini batch. The ``mini_batch `` is a list of tuples ``(x, y)``, and ``eta`` is the learning \n",
    "        rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b , delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b , delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w , delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw  for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases =  [b-(eta/len(mini_batch))*nb  for b, nb in zip(self.biases , nabla_b)]\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C_x. \n",
    "        ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of numpy arrays, similar to ``self.biases`` \n",
    "        and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * self.sigmoid_p(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book. Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = self.sigmoid_p(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        #####################################################################################################\n",
    "        \"\"\"Return the number of test inputs for which the neural network outputs the correct result. Note \n",
    "        that the neural network's output is assumed to be the index of whichever neuron in the final layer \n",
    "        has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    # Miscellaneous functions \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0/( 1.0 + np.exp(-z) )\n",
    "    def sigmoid_p(self, z):\n",
    "        '''The first derivative of sigmoid function'''\n",
    "        return 1.0/( 1.0 + np.exp(-z) )/( 1.0 + np.exp(z) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 0 / 10000\n"
     ]
    }
   ],
   "source": [
    "# Loading data from MNIST\n",
    "training_data, validation_data, test_data = load_data_wrapper()\n",
    "training_data = list(training_data)\n",
    "net = Network([784, 8, 10])\n",
    "net.SGD(training_data, epochs= 5, mini_batch_size= 10, eta = 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10,2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
